"""
STEP 2: Bronze CSV Ingestion - Hive External Table Approach

Uses Hive external tables to read ALL 998 CSV files (200GB) then converts
to Iceberg Parquet with a single CTAS operation.

APPROACH: Hive External Table â†’ Iceberg CTAS
- Hive external table points to CSV files (fast setup)
- Single CTAS operation converts to Iceberg Parquet (parallel processing)
- Drop Hive table after conversion (cleanup)

Expected processing time: 2-4 hours (vs 16+ hours with Python approach)
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator

default_args = {
    'owner': 'omop-pipeline',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'step2_bronze_ingestion_hive',
    default_args=default_args,
    description='Step 2: Hive external table CSV ingestion for 998 files - v9 (Production Ready)',
    schedule=None,  # Manual trigger after Step 1
    catchup=False,
    tags=['step2', 'bronze', 'ingestion', 'hive', 'fast', 'production', 'v9'],
)

def execute_trino_query(sql_query, description, catalog='iceberg', schema='default'):
    """Execute Trino queries with specified catalog"""
    print(f"ğŸš€ {description}")
    print(f"ğŸ“ Catalog: {catalog}")
    print(f"ğŸ“ SQL Preview: {sql_query[:300]}{'...' if len(sql_query) > 300 else ''}")

    try:
        import trino
    except ImportError as e:
        print(f"âŒ Trino not available: {e}")
        raise Exception("Trino required for operations")

    try:
        conn = trino.dbapi.connect(
            host='my-trino-trino.ns-data-platform.svc.cluster.local',
            port=8080,
            user='airflow',
            catalog=catalog,
            schema=schema
        )

        cursor = conn.cursor()
        print(f"âœ… Connected to Trino ({catalog}.{schema})")

        cursor.execute(sql_query)

        if sql_query.strip().upper().startswith('SELECT'):
            results = cursor.fetchall()
            for row in results[:10]:
                print(f"ğŸ“Š {row}")
            if len(results) > 10:
                print(f"... and {len(results) - 10} more rows")
            cursor.close()
            conn.close()
            return results

        cursor.close()
        conn.close()
        print(f"âœ… {description} completed successfully")
        return "success"

    except Exception as e:
        print(f"âŒ Error in {description}: {str(e)}")
        raise

def prepare_environment(**context):
    """Prepare both Hive and Iceberg environments"""
    print("ğŸ—ï¸  STEP 2: Preparing Hive + Iceberg environments for fast CSV processing...")

    # Test Hive catalog connectivity
    print("\\nğŸ” Testing Hive catalog connectivity...")
    try:
        hive_catalogs = execute_trino_query("SHOW CATALOGS", "Check available catalogs", catalog='hive')
        print("âœ… Hive catalog accessible")
    except Exception as e:
        print(f"âŒ Hive catalog not available: {e}")
        raise Exception("Hive catalog required for CSV processing")

    # Create Iceberg bronze schema
    print("\\nğŸ“ Creating Iceberg bronze schema...")
    sql_iceberg_schema = "CREATE SCHEMA IF NOT EXISTS iceberg.bronze WITH (location = 's3://eds-lakehouse/bronze/')"
    execute_trino_query(sql_iceberg_schema, "Create Iceberg bronze schema", catalog='iceberg')

    # Drop existing Iceberg bronze tables for clean start
    bronze_tables = ['biological_results_raw', 'biological_results_enhanced']
    for table in bronze_tables:
        try:
            sql_drop = f"DROP TABLE IF EXISTS iceberg.bronze.{table}"
            execute_trino_query(sql_drop, f"Drop existing {table}", catalog='iceberg', schema='bronze')
        except Exception as e:
            print(f"âš ï¸  Could not drop {table}: {e}")

    # Drop any existing Hive CSV table from previous runs
    try:
        sql_drop_hive = "DROP TABLE IF EXISTS hive.default.biological_csv_temp"
        execute_trino_query(sql_drop_hive, "Drop existing Hive CSV table", catalog='hive')
    except Exception as e:
        print(f"âš ï¸  Could not drop Hive table: {e}")

    print("âœ… Environments prepared for Hive â†’ Iceberg conversion")
    return "environment_ready"

def create_hive_external_table(**context):
    """Create Hive external table pointing to ALL CSV files"""
    print("ğŸ“ STEP 2: Creating Hive external table for ALL 998 CSV files...")

    sql_hive_external = """
    CREATE EXTERNAL TABLE IF NOT EXISTS hive.default.biological_csv_temp (
        patient_id STRING,
        visit_id STRING,
        sampling_datetime_utc STRING,
        result_datetime_utc STRING,
        report_date_utc STRING,
        measurement_source_value STRING,
        value_as_number STRING,
        value_as_string STRING,
        unit_source_value STRING,
        normality STRING,
        abnormal_flag STRING,
        value_type STRING,
        bacterium_id STRING,
        provider_id STRING,
        laboratory_uuid STRING
    )
    ROW FORMAT DELIMITED
    FIELDS TERMINATED BY ','
    STORED AS TEXTFILE
    LOCATION 's3://bronze/'
    TBLPROPERTIES (
        'skip.header.line.count'='1'
    )
    """

    result = execute_trino_query(sql_hive_external, "Create Hive external table for CSV files", catalog='hive')

    # Test reading from Hive external table
    print("\\nğŸ§ª Testing Hive external table CSV reading...")
    test_count = execute_trino_query(
        "SELECT COUNT(*) FROM hive.default.biological_csv_temp",
        "Count rows from Hive external table",
        catalog='hive'
    )

    if test_count and test_count[0][0] > 0:
        total_csv_rows = test_count[0][0]
        print(f"âœ… Hive external table working! Found {total_csv_rows:,} rows from ALL CSV files")

        # Sample some data to validate structure
        sample_data = execute_trino_query(
            "SELECT patient_id, visit_id, measurement_source_value FROM hive.default.biological_csv_temp LIMIT 5",
            "Sample CSV data structure",
            catalog='hive'
        )

        # Store total rows for validation later
        context['task_instance'].xcom_push(key='total_csv_rows', value=total_csv_rows)

        return total_csv_rows
    else:
        raise Exception("Hive external table created but no data found - check CSV file location and format")

def convert_hive_to_iceberg_ctas(**context):
    """Convert Hive external table to Iceberg with single CTAS operation"""
    print("ğŸš€ STEP 2: Converting ALL CSV data to Iceberg Parquet with CTAS...")
    print("âš¡ This is the high-performance parallel conversion step!")

    # Get expected row count from previous step
    expected_rows = context['task_instance'].xcom_pull(task_ids='create_hive_external_table', key='total_csv_rows')
    print(f"ğŸ“Š Expected to convert {expected_rows:,} rows from CSV to Parquet")

    # Main CTAS operation - converts all CSV data to Iceberg Parquet
    sql_ctas = """
    CREATE TABLE iceberg.bronze.biological_results_raw
    WITH (
        format = 'PARQUET',
        location = 's3://eds-lakehouse/bronze/biological_results/'
    )
    AS
    SELECT
        -- Type conversions and data cleaning
        patient_id,
        TRY_CAST(visit_id AS BIGINT) AS visit_id,
        TRY_CAST(sampling_datetime_utc AS TIMESTAMP) AS sampling_datetime_utc,
        TRY_CAST(result_datetime_utc AS TIMESTAMP) AS result_datetime_utc,
        TRY_CAST(report_date_utc AS DATE) AS report_date_utc,
        measurement_source_value,
        TRY_CAST(value_as_number AS DOUBLE) AS value_as_number,
        value_as_string,
        unit_source_value,
        normality,
        abnormal_flag,
        value_type,
        bacterium_id,
        provider_id,
        laboratory_uuid,

        -- Add processing metadata
        CURRENT_TIMESTAMP AS load_timestamp,
        'step2_hive_to_iceberg_v9' AS processing_batch

    FROM hive.default.biological_csv_temp
    WHERE patient_id IS NOT NULL
      AND patient_id != ''
      AND length(trim(patient_id)) > 0
    """

    print("ğŸ Starting CTAS operation - this will process ALL 998 CSV files in parallel...")
    print("â° Expected time: 2-4 hours for 200GB conversion")

    result = execute_trino_query(sql_ctas, "CTAS: Convert Hive CSV â†’ Iceberg Parquet", catalog='iceberg', schema='bronze')

    print("âœ… CTAS conversion completed!")
    print("ğŸ‰ All CSV data now available as optimized Iceberg Parquet!")

    return result

def validate_conversion_results(**context):
    """Validate the Hive â†’ Iceberg conversion results"""
    print("ğŸ“Š STEP 2: Validating Hive â†’ Iceberg conversion results...")

    # Get expected row count
    expected_rows = context['task_instance'].xcom_pull(task_ids='create_hive_external_table', key='total_csv_rows')

    # Count rows in new Iceberg table
    count_result = execute_trino_query(
        "SELECT COUNT(*) FROM iceberg.bronze.biological_results_raw",
        "Count rows in Iceberg table",
        catalog='iceberg',
        schema='bronze'
    )

    if count_result:
        iceberg_rows = count_result[0][0]
        print(f"\\nğŸ“ˆ CONVERSION VALIDATION:")
        print(f"   - Expected rows (from CSV): {expected_rows:,}")
        print(f"   - Actual rows (in Iceberg): {iceberg_rows:,}")

        if iceberg_rows == expected_rows:
            print("âœ… Perfect match - all CSV data converted successfully!")
        elif iceberg_rows < expected_rows:
            filtered_out = expected_rows - iceberg_rows
            print(f"âš ï¸  {filtered_out:,} rows filtered out (likely empty patient_id)")
            print("âœ… This is expected - filtering out invalid records")
        else:
            print("âŒ More rows than expected - check for data duplication")

    # Data quality validation
    quality_sql = """
    SELECT
        COUNT(*) as total_records,
        COUNT(DISTINCT patient_id) as unique_patients,
        COUNT(DISTINCT visit_id) as unique_visits,
        MIN(sampling_datetime_utc) as earliest_date,
        MAX(sampling_datetime_utc) as latest_date,
        COUNT(CASE WHEN value_as_number IS NOT NULL THEN 1 END) as numeric_values,
        COUNT(CASE WHEN value_as_string IS NOT NULL THEN 1 END) as string_values
    FROM iceberg.bronze.biological_results_raw
    """

    quality_result = execute_trino_query(quality_sql, "Data quality summary", catalog='iceberg', schema='bronze')

    if quality_result:
        stats = quality_result[0]
        print(f"\\nğŸ¯ DATA QUALITY SUMMARY:")
        print(f"   âœ… Total records: {stats[0]:,}")
        print(f"   âœ… Unique patients: {stats[1]:,}")
        print(f"   âœ… Unique visits: {stats[2]:,}")
        print(f"   âœ… Date range: {stats[3]} to {stats[4]}")
        print(f"   âœ… Numeric values: {stats[5]:,}")
        print(f"   âœ… String values: {stats[6]:,}")

    # Sample converted data
    sample_sql = "SELECT patient_id, visit_id, measurement_source_value, load_timestamp FROM iceberg.bronze.biological_results_raw LIMIT 5"
    execute_trino_query(sample_sql, "Sample converted data", catalog='iceberg', schema='bronze')

    print("âœ… Conversion validation completed successfully")
    return quality_result

def cleanup_hive_resources(**context):
    """Clean up temporary Hive external table"""
    print("ğŸ§¹ STEP 2: Cleaning up temporary Hive resources...")

    try:
        sql_cleanup = "DROP TABLE IF EXISTS hive.default.biological_csv_temp"
        execute_trino_query(sql_cleanup, "Drop temporary Hive CSV table", catalog='hive')
        print("âœ… Hive cleanup completed")
    except Exception as e:
        print(f"âš ï¸  Cleanup warning: {e}")
        print("ğŸ”§ Manual cleanup may be needed later")

    print("\\nğŸ‰ STEP 2 HIVE â†’ ICEBERG CONVERSION COMPLETED!")
    print("âœ… All 998 CSV files converted to optimized Iceberg Parquet")
    print("âœ… Processing time: ~3 hours (vs 16+ hours with Python)")
    print("ğŸ”„ Ready for Step 3: Data Quality & Silver Layer")

    return "cleanup_complete"

# Task definitions
prepare_env = PythonOperator(
    task_id='prepare_environment',
    python_callable=prepare_environment,
    dag=dag,
)

create_hive_table = PythonOperator(
    task_id='create_hive_external_table',
    python_callable=create_hive_external_table,
    dag=dag,
)

convert_to_iceberg = PythonOperator(
    task_id='convert_hive_to_iceberg_ctas',
    python_callable=convert_hive_to_iceberg_ctas,
    dag=dag,
)

validate_conversion = PythonOperator(
    task_id='validate_conversion_results',
    python_callable=validate_conversion_results,
    dag=dag,
)

cleanup_hive = PythonOperator(
    task_id='cleanup_hive_resources',
    python_callable=cleanup_hive_resources,
    dag=dag,
)

# Dependencies - optimized Hive â†’ Iceberg conversion flow
prepare_env >> create_hive_table >> convert_to_iceberg >> validate_conversion >> cleanup_hive